{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mev3n1zEveXS"
      },
      "source": [
        "# Twitter scraping and preprocessing of the data for Waseem\n",
        "In this notebook Twitter scraping based on having only tweet ids is done. Some of the preprocessing of the data is done as well, such as dropping the id column, dropping the rows of the tweets that API could not retrieve (deleted tweets, suspended / deleted accounts). \n",
        "\n",
        "This is a separate file because extracting tweets from Twitter takes a really long time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xU7RCLxwRe2"
      },
      "source": [
        "The first step in our process includes simply retrieiving the data set in question. In this case, Waseem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "kcQWmXxDvI0v",
        "outputId": "5bc17014-eaa7-4712-8b41-dd81137b9d1c"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "url_waseem_data = \"https://raw.githubusercontent.com/zeeraktalat/hatespeech/master/NAACL_SRW_2016.csv\"\n",
        "df_waseem = pd.read_csv(url_waseem_data)\n",
        "df_waseem.columns = [\"tweet\", \"class_label\"]\n",
        "df_waseem"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>class_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>572341498827522049</td>\n",
              "      <td>racism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>572340476503724032</td>\n",
              "      <td>racism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>572334712804384768</td>\n",
              "      <td>racism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>572332655397629952</td>\n",
              "      <td>racism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>575949086055997440</td>\n",
              "      <td>racism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16901</th>\n",
              "      <td>576359685843861505</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16902</th>\n",
              "      <td>576612926838046720</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16903</th>\n",
              "      <td>576771329975664640</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16904</th>\n",
              "      <td>560595245814267905</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16905</th>\n",
              "      <td>569363477095174145</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16906 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    tweet class_label\n",
              "0      572341498827522049      racism\n",
              "1      572340476503724032      racism\n",
              "2      572334712804384768      racism\n",
              "3      572332655397629952      racism\n",
              "4      575949086055997440      racism\n",
              "...                   ...         ...\n",
              "16901  576359685843861505        none\n",
              "16902  576612926838046720        none\n",
              "16903  576771329975664640        none\n",
              "16904  560595245814267905        none\n",
              "16905  569363477095174145        none\n",
              "\n",
              "[16906 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGIRMNJ5wg9Q"
      },
      "source": [
        "# Twitter scraping\n",
        "In this section, we will get the tweets having only tweet ids from Twitter using tweepy. This was a very challenging task because Twitter is very protective about authentication process. We tried multiple methods on how this can be done:\n",
        "\n",
        "\n",
        "1.   Using tweepy and get_status\n",
        "2.   Using TwitterAPI\n",
        "3.   Using tweepy's Client.\n",
        "4.   Using web scraping techniques (using Beautiful Soup or lxml).\n",
        "\n",
        "At first options 1 and 2 did not work because we did not have access to Twitter Elevated Account (we only had basic one which is called Essential) which is required to retieve tweet content from the id. We applied for the account and it got rejected. We had to reappeal in order to gain access and it took multiple days. In the meantime, we hopelessly searched for other alternatives. Option 3 did not work because it required v4 of tweepy that is still in the development (more to read about this issue [here](https://stackoverflow.com/questions/67978717/tweepy-3-10-0-attributeerror-module-tweepy-has-no-attribute-client) ). The last technique also did not work because of how protective Twitter is of the data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOnRcfzLx6ad",
        "outputId": "9ce65729-9b11-4c3e-e888-133dec68f390"
      },
      "source": [
        "!pip3 install tweepy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM1_-Mg4yWzA"
      },
      "source": [
        "import tweepy\n",
        "# The keys were replaced with empty strings for the protection of privacy on Github\n",
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\"\n",
        "  \n",
        "# authorization of consumer key and consumer secret\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  \n",
        "# set access to user's access key and access secret \n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "  \n",
        "# calling the api \n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWMIe4KPy7ZZ"
      },
      "source": [
        "def get_tweet_text(id):\n",
        "  try:\n",
        "    status = api.get_status(id)\n",
        "    return status.text\n",
        "  except Exception:\n",
        "    return \"ERROR\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUtuhGDhzPK-"
      },
      "source": [
        "Here we show how the function get_tweet_text works on one tweet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9fTcG8rzZ7h",
        "outputId": "4d9a5ff5-5bd9-44b6-99f5-ccce7ac044e6"
      },
      "source": [
        "print(get_tweet_text(572341498827522049))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbhLCU3AzvTH"
      },
      "source": [
        "# Waseem data \n",
        "In this section we will transform Waseem dataset to contain the tweet text as well using the function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkgqG2OKzuea"
      },
      "source": [
        "df_waseem[\"text\"] = df_waseem.tweet.apply(lambda x: get_tweet_text(x))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FUb_aiN0jI-"
      },
      "source": [
        "In this step we do some of the preprocessing of the data. We drop the error rows, make sure text column is before class_label column, and drop id column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDEYiP1I08qO"
      },
      "source": [
        "df_waseem = df_waseem[df_waseem.text != 'ERROR'] # drop error rows\n",
        "columns_titles = [\"text\",\"class_label\"] # swap the columns order and drop id column \n",
        "df_waseem=df_waseem.reindex(columns=columns_titles)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv3R6U_j1rfC"
      },
      "source": [
        "In the last step we save csv file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdD5_sf_1qew"
      },
      "source": [
        "df_waseem.to_csv(\"df_waseem_preprocessed.csv\", sep=' ')"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}