{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs for hate speech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
<<<<<<< HEAD
      "toc_visible": true
=======
      "authorship_tag": "ABX9TyOziKnPeJbN3YYFWD/zla1N",
      "include_colab_link": true
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuziamatysiak/cis419-hate-speech/blob/main/colab%20notebooks/CNNs_for_hate_speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OlwOMXv9XRP"
      },
      "source": [
        "# CNNs for hate speech detection\n",
        "In this notebook we will explore different CNN models to perform the task of hate speech detection on our datasets. This file is in a separate colab to provide more clarity on how our files and code are structured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guSCLtkKALj9"
      },
      "source": [
        "Firstly, let's just import the relevant data. Note that for CNNs we will use torch's method of tokenizing so we import untokenized version of our dataset but with already cleared tweet structure (no punctuation, unnecessary blank spaces, hashtags, retweets and links). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "w4GMTCxhOwO4",
        "outputId": "faae26c2-7d3e-4db5-d87f-84f3b7562dc9"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "df_dav = pd.read_csv('/content/df_davs.csv', sep=' ')\n",
        "df_dav = df_dav.drop([\"hate_speech\", \"count\", \"offensive_language\", \"neither\"], axis = 1)\n",
        "df_dav=df_dav.reindex(columns=['tweet', 'class'])\n",
        "df_dav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a woman you shouldnt complain about cleani...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>boy dats coldtyga dwn bad for cuffin dat hoe ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dawg RT You ever fuck a bitch and she start t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she look like a tranny</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The shit you hear about me might be true or i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>yous a muthafin lie right His TL is trash Now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>youve gone and broke the wrong heart baby and ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>young buck wanna eat dat nigguh like I aint fu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>youu got wild bitches tellin you lies</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>Ruffled Ntac Eileen Dahlia Beautiful color com...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  class\n",
              "0       As a woman you shouldnt complain about cleani...      2\n",
              "1       boy dats coldtyga dwn bad for cuffin dat hoe ...      1\n",
              "2       Dawg RT You ever fuck a bitch and she start t...      1\n",
              "3                                 she look like a tranny      1\n",
              "4       The shit you hear about me might be true or i...      1\n",
              "...                                                  ...    ...\n",
              "24778  yous a muthafin lie right His TL is trash Now ...      1\n",
              "24779  youve gone and broke the wrong heart baby and ...      2\n",
              "24780  young buck wanna eat dat nigguh like I aint fu...      1\n",
              "24781              youu got wild bitches tellin you lies      1\n",
              "24782  Ruffled Ntac Eileen Dahlia Beautiful color com...      2\n",
              "\n",
              "[24783 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Waseem dataset"
      ],
      "metadata": {
        "id": "sA5JXo18h4c4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "rSWvF_bmxDI2",
        "outputId": "1f9037d1-8165-4e7c-9c75-283ee809cc8b"
      },
      "source": [
        "df_was = pd.read_csv('/content/df_was.csv', sep=' ')\n",
        "df_was[\"class_label\"] = df_was['class_label'].fillna(\"none\")\n",
        "df_was['class_label'].loc[(df_was['class_label'] == \"racism\")] = 0\n",
        "df_was['class_label'].loc[(df_was['class_label'] == \"sexism\")] = 1\n",
        "df_was['class_label'].loc[(df_was['class_label'] == \"none\")] = 2\n",
        "df_was['text'] = df_was['text'].astype(str)\n",
        "df_was"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['allow', 'word', 'to', 'evolve', 'to', 'its',...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['That', 'didnt', 'take', 'much', 'httptc']</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['Looks', 'like', 'shit', 'cuntandandrenot', '...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['Its', 'not', 'the', 'only', 'thing', 'shes',...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['mkr', 'if', 'we', 'have', 'to', 'see', 'Kat'...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2738</th>\n",
              "      <td>['Is', 'My', 'Kitchen', 'Rules', 'about', 'coo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2739</th>\n",
              "      <td>['SeeI', 'specifically', 'left', 'those', 'out...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2740</th>\n",
              "      <td>['Im', 'not', 'sexist', 'but', 'the', 'Mens', ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2741</th>\n",
              "      <td>['JUST', 'FELT', 'THE', 'NEED', 'EXPRESS', 'TH...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2742</th>\n",
              "      <td>['Im', 'not', 'sexist', 'but', 'most', 'women'...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2743 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  class_label\n",
              "0     ['allow', 'word', 'to', 'evolve', 'to', 'its',...            2\n",
              "1           ['That', 'didnt', 'take', 'much', 'httptc']            1\n",
              "2     ['Looks', 'like', 'shit', 'cuntandandrenot', '...            1\n",
              "3     ['Its', 'not', 'the', 'only', 'thing', 'shes',...            1\n",
              "4     ['mkr', 'if', 'we', 'have', 'to', 'see', 'Kat'...            2\n",
              "...                                                 ...          ...\n",
              "2738  ['Is', 'My', 'Kitchen', 'Rules', 'about', 'coo...            1\n",
              "2739  ['SeeI', 'specifically', 'left', 'those', 'out...            2\n",
              "2740  ['Im', 'not', 'sexist', 'but', 'the', 'Mens', ...            1\n",
              "2741  ['JUST', 'FELT', 'THE', 'NEED', 'EXPRESS', 'TH...            1\n",
              "2742  ['Im', 'not', 'sexist', 'but', 'most', 'women'...            1\n",
              "\n",
              "[2743 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Founta dataset"
      ],
      "metadata": {
        "id": "lDsR5kOah7im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_founta = pd.read_csv('/content/df_founta.csv', sep=' ')\n",
        "df_founta[\"class_label\"] = df_founta['class_label'].fillna(\"none\")\n",
        "df_founta['class_label'].loc[(df_founta['class_label'] == \"abusive\")] = 0\n",
        "df_founta['class_label'].loc[(df_founta['class_label'] == \"hateful\")] = 1\n",
        "df_founta['class_label'].loc[(df_founta['class_label'] == \"none\")] = 2\n",
        "df_founta['text'] = df_founta['text'].astype(str)\n",
        "\n",
        "# Defining which classes we will be dropping when testing\n",
        "df_founta = df_founta[df_founta.class_label != \"spam\"]\n",
        "df_founta = df_founta[df_founta.class_label != \"normal\"]\n",
        "df_founta = df_founta.sample(frac = 1)\n",
        "df_founta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TR_P7A8xYVfc",
        "outputId": "2a009120-66d5-4faa-82e4-d7b8ce27a6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94992</th>\n",
              "      <td>I hate a frontin ass person stay from around ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64382</th>\n",
              "      <td>Fucking kill me now Jesus Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70831</th>\n",
              "      <td>Adam and Eves promo art aesthetic for Nier Au...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95788</th>\n",
              "      <td>SCREW ryan amp his agenda amp Idiotic process...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59618</th>\n",
              "      <td>I just drafted Emre Can amp Divock Origi on fo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>You tweakin lul bro its some bad bitches out hea</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94477</th>\n",
              "      <td>Is that your actual argument Me If youre just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56410</th>\n",
              "      <td>If you ask me for advice imma tell you what y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24101</th>\n",
              "      <td>ARIANA HIT 100M FOLLOWERS ON INSTAGRAM WHAT A...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75133</th>\n",
              "      <td>Megan Fox was a bad bitch httpstco5A0qORDyU6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32115 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text class_label\n",
              "94992   I hate a frontin ass person stay from around ...           0\n",
              "64382                   Fucking kill me now Jesus Christ           0\n",
              "70831   Adam and Eves promo art aesthetic for Nier Au...           0\n",
              "95788   SCREW ryan amp his agenda amp Idiotic process...           0\n",
              "59618  I just drafted Emre Can amp Divock Origi on fo...           0\n",
              "...                                                  ...         ...\n",
              "942     You tweakin lul bro its some bad bitches out hea           0\n",
              "94477   Is that your actual argument Me If youre just...           0\n",
              "56410   If you ask me for advice imma tell you what y...           0\n",
              "24101   ARIANA HIT 100M FOLLOWERS ON INSTAGRAM WHAT A...           0\n",
              "75133       Megan Fox was a bad bitch httpstco5A0qORDyU6           0\n",
              "\n",
              "[32115 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-Nr7rVB6u2"
      },
      "source": [
<<<<<<< HEAD
        "Now let's import relevant libraries. "
=======
        "Now let's import relevant for us libraries. "
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y21A6SzvP7wk"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFKZ6jfDEFye"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt0TDcg8CBVP"
      },
      "source": [
        "We will know get the split of the data for the Davidson dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad4T8-VQQJpD"
      },
      "source": [
        "X_train, X_valid, Y_train, Y_valid= train_test_split(df_dav['tweet'].tolist(),\n",
        "                                                      df_dav['class'].tolist(),\n",
        "                                                      test_size=0.3)\n",
        "train_data =list(zip(Y_train,X_train))\n",
        "valid_data =list(zip(Y_valid,X_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EkOgL0TCLEb"
      },
      "source": [
        "Let's use torch's get_tokenizer method now to tokenize our dataset. It will be tokenized into inidices in the basic english dictionary so that we can use CNNs for predictions here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HJW26w9Qbjg"
      },
      "source": [
        "tzr = get_tokenizer('basic_english')\n",
        "def torch_tokenizer(iterator):\n",
        "  for _, text in iterator:\n",
        "        yield tzr(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA06mVvQRST-"
      },
      "source": [
        "dataset_copy = train_data\n",
        "transformed = build_vocab_from_iterator(torch_tokenizer(dataset_copy), specials=[\"<unk>\"])\n",
        "transformed.set_default_index(transformed[\"<unk>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFRtxRY4R91H"
      },
      "source": [
        "text_to_index = lambda x: transformed(tzr(x))\n",
        "label_trans = lambda x: int(x) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRT6WrHNCaNx"
      },
      "source": [
        "Let's see how it works in practice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEVlYFvXS-ih",
        "outputId": "f82b92aa-6390-4036-d4ac-8e15b730eb6e"
      },
      "source": [
        "text_to_index('hello world')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1502, 379]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSd4OIoXCgKe"
      },
      "source": [
        "Let's introduce the first CNN model. We will be using embedding bag, 3 linear layers, and relu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrtpcJFATKhl"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class CNNModel1(nn.Module):\n",
        "\n",
        "    def __init__(self, transformer_size, dim_in, dim_out):\n",
        "        super(CNNModel1, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(transformer_size, dim_in, sparse=True)\n",
        "        self.lin1 = nn.Linear(dim_in, 64)\n",
        "        self.lin2 = nn.Linear(64,16)\n",
        "        self.lin3 = nn.Linear(16, dim_out)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        r = 0.5\n",
        "        self.embedding.weight.data.uniform_(r, r)\n",
        "        nn.init.xavier_uniform_(self.lin1.weight)\n",
        "        nn.init.zeros_(self.lin1.bias)\n",
        "        nn.init.kaiming_normal_(self.lin2.weight)\n",
        "        nn.init.zeros_(self.lin2.bias)\n",
        "        self.lin3.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.lin3.bias)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        x = self.relu(self.lin1(embedded))\n",
        "        x = self.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgjcBHkHC1po"
      },
      "source": [
        "In the second model, we will be using embedding bag, 4 linear layers, dropout, and relu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V02wf8h8F328"
      },
      "source": [
        "class CNNModel2(nn.Module):\n",
        "      def __init__(self, transformer_size, dim_in, dim_out):\n",
        "        super(CNNModel2, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(transformer_size, dim_in, sparse=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear_1 = torch.nn.Linear(dim_in, 100)\n",
        "        self.linear_2 = torch.nn.Linear(100, 64)\n",
        "        self.linear_3 = torch.nn.Linear(64, 16)\n",
        "        self.linear_4 = torch.nn.Linear(16, dim_out)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "        self.init_weights()\n",
        "\n",
        "      def init_weights(self):\n",
        "          r = 0.5\n",
        "          self.embedding.weight.data.uniform_(r, r)\n",
        "          nn.init.xavier_uniform_(self.linear_1.weight)\n",
        "          nn.init.zeros_(self.linear_1.bias)\n",
        "          nn.init.kaiming_normal_(self.linear_2.weight)\n",
        "          nn.init.zeros_(self.linear_2.bias)\n",
        "          self.linear_3.weight.data.uniform_(-r, r)\n",
        "          nn.init.zeros_(self.linear_3.bias)\n",
        "          self.linear_4.weight.data.uniform_(-r, r)\n",
        "          nn.init.zeros_(self.linear_4.bias)\n",
        "\n",
        "      def forward(self, text, offsets):\n",
        "          embedded = self.embedding(text, offsets)\n",
        "          x = self.relu(self.linear_1(embedded))\n",
        "          x = self.relu(self.linear_2(x))\n",
        "          x = self.dropout(self.linear_3(x))\n",
        "          x = self.linear_4(x)\n",
        "          return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y03EOhODASR"
      },
      "source": [
        "In the third CNN model, we will be using embedding bag, max pool, batch norm, 2 ;linear layers and relu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODIdYJEW21In"
      },
      "source": [
        "class CNNModel3(torch.nn.Module):\n",
        "    def __init__(self, transformer_size, dim_in, dim_out):\n",
        "        super(CNNModel3, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(transformer_size, dim_in, sparse=True)\n",
        "        self.max_pool2d = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.batch_norm = torch.nn.BatchNorm1d(num_features=64)\n",
        "        self.linear_1 = torch.nn.Linear(dim_in, 128)\n",
        "        self.linear_2 = torch.nn.Linear(64, dim_out)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        r = 0.5\n",
        "        self.embedding.weight.data.uniform_(r, r)\n",
        "        nn.init.kaiming_normal_(self.linear_2.weight)\n",
        "        nn.init.zeros_(self.linear_2.bias)\n",
        "        self.linear_1.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_1.bias)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        x = self.relu(self.linear_1(embedded))\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = x.reshape(x.size(0), -1) \n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR_mbyAjDJ9Z"
      },
      "source": [
        "In the fourth CNN model, we will use embedding bag, 1d convolutions layer, 3 linear layers, dropout and relu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGxk4GCg9ocp"
      },
      "source": [
        "class CNNModel4(torch.nn.Module):\n",
        "    def __init__(self, transformer_size, dim_in, dim_out):\n",
        "        super(CNNModel4, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(transformer_size, dim_in, sparse=True)\n",
        "        self.conv_1 = torch.nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool2d = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.linear_1 = torch.nn.Linear(dim_in, 128)\n",
        "        self.linear_2 = torch.nn.Linear(2048, 16)\n",
        "        self.linear_3 = torch.nn.Linear(16, dim_out)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        r = 0.5\n",
        "        self.embedding.weight.data.uniform_(r, r)\n",
        "        nn.init.xavier_uniform_(self.conv_1.weight)\n",
        "        nn.init.zeros_(self.conv_1.bias)\n",
        "        nn.init.kaiming_normal_(self.linear_2.weight)\n",
        "        nn.init.zeros_(self.linear_2.bias)\n",
        "        self.linear_1.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_1.bias)\n",
        "        self.linear_3.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_3.bias)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        x = self.relu(self.linear_1(embedded))\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.max_pool2d(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.relu(self.linear_2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aHcY9gCDVJO"
      },
      "source": [
        "In the fifth CNN model we used embedding bag, max pool 1d convolutional layer 1d, 5 linear layers, dropout and relu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEEFJfajDTvY"
      },
      "source": [
        "class CNNModel5(torch.nn.Module):\n",
        "    def __init__(self, transformer_size, dim_in, dim_out):\n",
        "        super(CNNModel5, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(transformer_size, dim_in, sparse=True)\n",
        "        self.conv_1 = torch.nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool2d = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.linear_1 = torch.nn.Linear(dim_in, 128)\n",
        "        self.linear_2 = torch.nn.Linear(2048, 1024)\n",
        "        self.linear_3 = torch.nn.Linear(1024, 512)\n",
        "        self.linear_4 = torch.nn.Linear(512, 128)\n",
        "        self.linear_5 = torch.nn.Linear(128, dim_out)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        r = 0.5\n",
        "        self.embedding.weight.data.uniform_(r, r)\n",
        "        nn.init.xavier_uniform_(self.conv_1.weight)\n",
        "        nn.init.zeros_(self.conv_1.bias)\n",
        "        nn.init.kaiming_normal_(self.linear_2.weight)\n",
        "        nn.init.zeros_(self.linear_2.bias)\n",
        "        self.linear_1.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_1.bias)\n",
        "        self.linear_3.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_3.bias)\n",
        "        self.linear_4.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_4.bias)\n",
        "        self.linear_5.weight.data.uniform_(-r, r)\n",
        "        nn.init.zeros_(self.linear_5.bias)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        x = self.relu(self.linear_1(embedded))\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.max_pool2d(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.relu(self.linear_2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.relu(self.linear_4(x))\n",
        "        x = self.linear_5(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tls9tbmNDhh0"
      },
      "source": [
        "This function is a solution we found for making sure CNNs work well with text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a25do0LpVcUk"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_trans(_label))\n",
        "         processed_text = torch.tensor(text_to_index(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezae25odDr3L"
      },
      "source": [
        "In this function, we train the provided model and get its accuracy for a specific dataset. We use CrossEntropyLoss as criterion and SGD as an optimizer. We can differentiate training and evaluation steps for each epoch. Note that training CNNSs for text is identical to training CNNs for images, however, the whole process of making them ready to be processed differs a lot from the images CNNs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ8f2nM09Fz6"
      },
      "source": [
        "import torch.optim as optim\n",
        "def train_and_get_accuracy(train_d, valid_d, model, epochs = 10, lr = 0.001, BATCH_SIZE = 128):\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  total_accu = None\n",
        "\n",
        "  train_dataloader = DataLoader(train_d, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "  valid_dataloader = DataLoader(valid_d, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # training step\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
        "      optimizer.zero_grad()\n",
        "      predicted_label = model(text, offsets)\n",
        "      loss = criterion(predicted_label, label)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "      optimizer.step()\n",
        "      total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "    # evaluation step\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(valid_dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "    accu_val = total_acc/total_count\n",
        "    \n",
        "  print('Final accuracy: {:f} '.format(accu_val))\n",
        "  return accu_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
<<<<<<< HEAD
      "source": [
        "### Davidson CNN"
      ],
      "metadata": {
        "id": "mMxPiUQ5cRt2"
      }
    },
    {
      "cell_type": "markdown",
=======
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
      "metadata": {
        "id": "cHu5_fhdEh53"
      },
      "source": [
        "Let's run **CNN Model1** on the **Davidson** dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a5Ljx3L91dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7252c355-97f7-4520-f7b8-ef76aebeb7cb"
      },
      "source": [
        "model = CNNModel1(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn1_accu_dav = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn1_accu_dav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.773638 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7736381977135172"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fmozR-WEo9w"
      },
      "source": [
        "Now **CNN Model2** on **Davidson** as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9_IEzyVHLFi",
        "outputId": "17230b1a-d56d-4c4c-e5ee-98e042d8c1d8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2 = CNNModel2(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn2_accu_dav = train_and_get_accuracy(train_data, valid_data, model2, epochs = 100)\n",
        "cnn2_accu_dav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.773638 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7736381977135172"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8LjzMn3ExWx"
      },
      "source": [
        "Followed by **Model3** on **Davidson**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anw2czkd31aq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1850a48c-3643-4e0d-810f-d190fb07c4a4"
      },
      "source": [
        "model = CNNModel3(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn3_accu_dav = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn3_accu_dav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.783053 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7830531271015467"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idet7U4EE17J"
      },
      "source": [
        "Next **Model4** on **Davidson**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6Iu_KkM-PWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e481fe01-56c5-45b4-93c6-5e103bfbddf0"
      },
      "source": [
        "model = CNNModel4(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn4_accu_dav = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn4_accu_dav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.773638 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7736381977135172"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f3tazkUE6Bb"
      },
      "source": [
        "Finally, **Model5** on the **Davidson** dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2naZcjmZELBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69839a2c-7f30-440b-fb1b-ed41cd9b8e69"
      },
      "source": [
        "model = CNNModel5(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn5_accu_dav = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn5_accu_dav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.773638 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7736381977135172"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
<<<<<<< HEAD
      "source": [
        "### Waseem CNN "
      ],
      "metadata": {
        "id": "vb5XscO0cNR4"
      }
    },
    {
      "cell_type": "markdown",
=======
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
      "metadata": {
        "id": "fvGARdfPFAOX"
      },
      "source": [
        "Now let's get **Waseem** data ready. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4az_3EUey3Xg"
      },
      "source": [
        "X_train, X_valid, Y_train, Y_valid= train_test_split(df_was['text'].tolist(),\n",
        "                                                      df_was['class_label'].tolist(),\n",
        "                                                      test_size=0.3)\n",
        "train_data =list(zip(Y_train,X_train))\n",
        "valid_data =list(zip(Y_valid,X_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWhoYgatzLfB"
      },
      "source": [
        "dataset_copy = train_data\n",
        "transformed = build_vocab_from_iterator(torch_tokenizer(dataset_copy), specials=[\"<unk>\"])\n",
        "transformed.set_default_index(transformed[\"<unk>\"])\n",
        "text_to_index = lambda x: transformed(tzr(x))\n",
        "label_trans = lambda x: int(x) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQIqXZyEFIWO"
      },
      "source": [
        "Now we call **CNNModel1** on the **Waseem** dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBf3FYPBzkgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdd41eb-baa3-471a-d435-28b859744d16"
      },
      "source": [
        "model = CNNModel1(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn1_accu_was = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn1_accu_was"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.692588 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.692588092345079"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc4XczclFOWH"
      },
      "source": [
        "Now **CNNModel2** on the **Waseem** dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkDCrqP0zmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325dcf78-575d-4617-9f17-51add75edfaf"
      },
      "source": [
        "model = CNNModel2(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn1_accu_was = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn1_accu_was"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.003645 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0036452004860267314"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFtTpYUFVgX"
      },
      "source": [
        "Followed by **Model3** on **Waseem**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZloEInp7eTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ae962b-680c-44cc-89fa-70933b9a2f08"
      },
      "source": [
        "model = CNNModel3(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn3_accu_was = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn3_accu_was"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.184690 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18469015795868773"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSmV4C08FbcL"
      },
      "source": [
        "Now, **Model 4** for **Waseem**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESJEHWeBTGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26dda94-6d0f-42c8-891e-000a00492b34"
      },
      "source": [
        "model = CNNModel4(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn4_accu_was = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn4_accu_was"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.303767 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3037667071688943"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4CSkedBFfI3"
      },
      "source": [
        "And finally **Model5** on **Waseem**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-wTP7IEXNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720b60f7-dece-4495-a91b-f5f06b26e3e0"
      },
      "source": [
        "model = CNNModel5(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn5_accu_was = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn5_accu_was"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.003645 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0036452004860267314"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo4n6-8ALmRk"
      },
      "source": [
        "Models on Waseem dataset perform poorly because there is only 2k instances. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN-aIHRowJnZ"
      },
      "source": [
        "We realized that accuracy is fairly low in comparison to models trained in the CIS419_Project (it's in the 90%+ there). Hence, we decided the abandon calculating recall, precision, and f1-score for our CNNs because the results from other models are better. "
      ]
    },
    {
<<<<<<< HEAD
      "cell_type": "markdown",
      "source": [
        "### Founta CNN"
      ],
      "metadata": {
        "id": "9dz8kh55cW68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get **Founta** data ready. "
      ],
      "metadata": {
        "id": "2tPAGyIvhgVV"
      }
    },
    {
=======
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
      "cell_type": "code",
      "source": [
        "X_train, X_valid, Y_train, Y_valid= train_test_split(df_founta['text'].tolist(),\n",
        "                                                      df_founta['class_label'].tolist(),\n",
        "                                                      test_size=0.3)\n",
        "train_data =list(zip(Y_train,X_train))\n",
        "valid_data =list(zip(Y_valid,X_valid))"
      ],
      "metadata": {
<<<<<<< HEAD
        "id": "jfuTTZJKcevB"
=======
        "id": "WAN4-utaw9PR"
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
<<<<<<< HEAD
        "Let's run **CNN Model1** on the **Founta** dataset. "
=======
        "# TODO(Amy): Founta"
>>>>>>> bda26353349f0b491cce654f691404423a1a6546
      ],
      "metadata": {
        "id": "d7G4jYHuhaJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy = train_data\n",
        "transformed = build_vocab_from_iterator(torch_tokenizer(dataset_copy), specials=[\"<unk>\"])\n",
        "transformed.set_default_index(transformed[\"<unk>\"])\n",
        "text_to_index = lambda x: transformed(tzr(x))\n",
        "label_trans = lambda x: int(x) "
      ],
      "metadata": {
        "id": "hTqjfpgTchmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we call **CNNModel1** on the **Founta** dataset. "
      ],
      "metadata": {
        "id": "6YXd8LZ7hpBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel1(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn1_accu_founta = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn1_accu_founta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnDZ9fGGcjBA",
        "outputId": "b3ebaac6-96ed-4026-887c-fa68838322ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.837260 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8372599896211728"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call **CNNModel2** on the **Founta** dataset. "
      ],
      "metadata": {
        "id": "DRtrdNdJhr1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel2(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn2_accu_founta = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn2_accu_founta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPS-47o4cmep",
        "outputId": "d7698385-6d5a-402a-ce2c-a43d7456cfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.837260 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8372599896211728"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call **CNNModel3** on the **Founta** dataset. "
      ],
      "metadata": {
        "id": "JESTpsUrht8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel3(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn3_accu_founta = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn3_accu_founta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGuVmC2gcnln",
        "outputId": "f9e908c5-1f41-49ae-bb2c-c5ebbcb27a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.730254 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7302542812662169"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call **CNNModel4** on the **Founta** dataset. "
      ],
      "metadata": {
        "id": "w9fWwtZjhvDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel4(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn4_accu_founta = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn4_accu_founta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDgx8VjZco68",
        "outputId": "c577c2fd-4d89-4831-bc33-d20615abc8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.837260 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8372599896211728"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call **CNNModel5** on the **Founta** dataset. "
      ],
      "metadata": {
        "id": "anMf1lgEhwMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel5(len(transformed), 1000, len(set([label for (label, text) in dataset_copy]))).to(device)\n",
        "cnn5_accu_founta = train_and_get_accuracy(train_data, valid_data, model)\n",
        "cnn5_accu_founta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiCtEO2VcqFp",
        "outputId": "b02eab2d-f171-4b40-ea91-a4914ad17492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 0.837260 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8372599896211728"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}